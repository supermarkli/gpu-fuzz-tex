\documentclass[letterpaper,twocolumn,10pt,review,anonymous]{article}

\usepackage{usenix}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\input{./files/util.tex}
\usepackage{subcaption}
\begin{document}

\title{GPU-Fuzz: A Constraint-Guided Fuzzer for Finding Memory Errors in GPU-Accelerated Deep Learning Frameworks}

\author{NAME}


\maketitle
\begin{abstract}
GPU memory errors represent an imminent and important threat to the reliability of deep learning (DL) frameworks, leading to system crashes and silent data corruption. However, discovering these bugs is challenging because existing fuzzers for DL systems focus on the neural network structure to find compiler logic errors. While effective for their purpose, they do not systematically explore the intricate operator parameter space where conditions that trigger low-level GPU memory errors lie. This paper introduces \textsc{Gpu-Fuzz}, a fuzzer specifically engineered to address this gap. Unlike structure-oriented fuzzers, \textsc{Gpu-Fuzz} models the complex relationships between operator parameters as formal constraints. It then uses a constraint solver to generate test cases that probe boundary conditions prone to memory errors in GPU kernels. By applying \textsc{Gpu-Fuzz} to major frameworks like PyTorch and TensorFlow, we discovered 14 previously unknown bugs, the majority of which are critical memory-related errors. Our work demonstrates that focusing on the operator parameter space is a highly effective and complementary approach for securing the foundational layers of modern DL systems.
\end{abstract}

\input{./files/1_intro.tex}
\input{./files/2_prel.tex}
\input{./files/3_design.tex}
\input{./files/4_impl.tex}
\input{./files/5_eval.tex}
\input{./files/6_discussion.tex}
\input{./files/7_related_work.tex}
\input{./files/8_disclosure.tex}
\input{./files/9_conclusion.tex}


\bibliographystyle{plainnat}
\bibliography{ref}

\appendix
\input{./files/appendix.tex}

\end{document}
\endinput
