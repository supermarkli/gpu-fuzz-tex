\section{Evaluation}
\label{sec:eval}

In this section, we evaluate the effectiveness and efficiency of \textsc{GPU-Fuzz}. We aim to answer the following research questions (RQs):
\begin{itemize}
    \item \textbf{RQ1:} How effective is \textsc{GPU-Fuzz} in uncovering real-world bugs in major deep learning frameworks?
    \item \textbf{RQ2:} How does \textsc{GPU-Fuzz} compare with state-of-the-art structure-level fuzzers in terms of test case generation and bug uncovery, particularly for GPU memory errors?
\end{itemize}

\subsection{Experimental Setup}
\label{sec:setup}
All experiments were conducted on a server with the configuration detailed in Table~\ref{tbl:setup}. We established isolated Conda environments for each of the three target frameworks (PyTorch, TensorFlow, and PaddlePaddle) to manage their specific dependencies. The core hardware, operating system, and NVIDIA driver were consistent across all tests.

\begin{table}[h!]
\centering
\caption{Experimental Environment Configuration.}
\label{tbl:setup}
\footnotesize
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Component} & \textbf{Specification} \\ \midrule
\multicolumn{2}{@{}l}{\textbf{Hardware}} \\
\hspace{1em}CPU & 2 x Intel Xeon Silver 4510 \\
\hspace{1em}GPU & NVIDIA H100 PCIe \\
\midrule
\multicolumn{2}{@{}l}{\textbf{Software (Common)}} \\
\hspace{1em}Operating System & Ubuntu 24.04.2 LTS \\
\hspace{1em}NVIDIA Driver & 580.82.07 \\
\hspace{1em}CUDA Runtime & 12.8.93 \\
\hspace{1em}Python & 3.11.13 \\
\midrule
\multicolumn{2}{@{}l}{\textbf{Frameworks (in separate Conda environments)}} \\
\hspace{1em}PyTorch & PyTorch 2.3.1+cu121 with cuDNN 8.9.2.26 \\
\hspace{1em}TensorFlow & 2.20.0, with cuDNN 9.13.0.50 \\
\hspace{1em}PaddlePaddle & 2.6.1 \\
\bottomrule
\end{tabular}%
\end{table}

\subsection{Bug Discovery Effectiveness}

Over the course of our evaluation, \textsc{GPU-Fuzz} uncovered a total of 13 previously unknown bugs across the three frameworks. Table~\ref{tbl:bugs} presents a summary of these findings. The bugs span a range of failure modes, from low-level memory corruption to API-level exceptions. We identified 8 distinct memory access violations (e.g., out-of-bounds or misaligned reads/writes). Among these, 6 were silent memory corruptions (Bug\textsubscript{2}, Bug\textsubscript{5}, Bug\textsubscript{6}, Bug\textsubscript{7}, Bug\textsubscript{8}, Bug\textsubscript{12}) that do not trigger any API-level crash and are only detectable with specialized tools like compute-sanitizer~\cite{nvidia2023compsan}. At the time of writing, we have reported all findings by opening issues in the corresponding repositories.

\begin{table*}[htbp]
\centering
\caption{Summary of Bugs Discovered by \textsc{GPU-Fuzz}.}
\label{tbl:bugs}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
\textbf{ID} & \textbf{Framework} & \textbf{Operator} & \textbf{Bug Type} & \textbf{Root Cause} & \textbf{Failure Mode} \\ \midrule
\textbf{Bug\textsubscript{1}} & PyTorch & \texttt{conv\_transpose2d} & OOB Global Write & Incorrect grid dimension calculation & GPU-Level Exception (CUBLAS) \\
\textbf{Bug\textsubscript{2}} & PyTorch & \texttt{bmm\_sparse} & Misaligned Global Write & Incorrect pointer arithmetic in CUSPARSE & Silent Memory Corruption \\
\textbf{Bug\textsubscript{3}} & PaddlePaddle & \texttt{conv2d\_transpose} & Precondition Violation & Integer overflow in tensor dimension calculation & CPU-Side Assert \\
\textbf{Bug\textsubscript{4}} & PaddlePaddle & \texttt{conv3d\_transpose} & Illegal Instruction & Invalid parameters passed to cuDNN kernel & GPU-Level Exception (cuDNN) \\
\textbf{Bug\textsubscript{5}} & PyTorch & \texttt{adaptive\_avg\_pool2d} & OOB Global Write & Flawed boundary checks in CUDA kernel & Silent Memory Corruption \\
\textbf{Bug\textsubscript{6}} & PyTorch & \texttt{replication\_pad2d} & OOB Global Write & Incorrect grid dimension calculation & Silent Memory Corruption \\
\textbf{Bug\textsubscript{7}} & PyTorch & \texttt{adaptive\_max\_pool2d} & OOB Global Write & Flawed boundary checks in CUDA kernel & Silent Memory Corruption \\
\textbf{Bug\textsubscript{8}} & TensorFlow & \texttt{Conv2D} & OOB Global Read & Incorrect index calculation in kernel & Silent Read / Downstream Crash \\
\textbf{Bug\textsubscript{9}} & TensorFlow & \texttt{Conv2D} & Integer Overflow & Overflow in launch config calculation & CPU-Side Assert \\
\textbf{Bug\textsubscript{10}} & PaddlePaddle & \texttt{conv2d\_transpose} & Bad API Parameter & Invalid parameter combination passed to cuDNN & GPU-Level Exception (cuDNN) \\
\textbf{Bug\textsubscript{11}} & PaddlePaddle & \texttt{conv2d\_transpose} & Invalid Launch Config & Incorrect grid/block dimension calculation & GPU-Level Exception (CUDA) \\
\textbf{Bug\textsubscript{12}} & PyTorch & \texttt{conv\_transpose3d} & OOB Shared Read & Incorrect index calculation for shared memory & Silent Memory Corruption \\
\textbf{Bug\textsubscript{13}} & PyTorch & \texttt{reflection\_pad1d} & Invalid Launch Config & Integer overflow in \texttt{torch.compile} symint logic & GPU-Level Exception (CUDA) \\
\bottomrule
\end{tabular}
}
\end{table*}

\parh{Bug Patterns.} Our findings reveal important patterns across three distinct failure modes:
\begin{itemize}
    \item \textbf{Silent Memory Corruption:} The most critical category, where out-of-bounds or misaligned memory access occurs without causing any API-level error. These are the most insidious bugs as they can lead to silent data corruption and are only detectable with low-level memory debuggers.
    \item \textbf{GPU-Level Exceptions:} The second category, where invalid parameters or configurations cause CUDA, cuDNN, or CUBLAS libraries to return an error, which is then typically caught and reported by the framework.
    \item \textbf{CPU-Side Asserts:} The final category, where issues like integer overflows occur on the CPU during the calculation of kernel parameters, preventing the GPU launch altogether.
\end{itemize}
\noindent A common root cause across all frameworks was incorrect grid dimension calculations or flawed boundary checks, with transposed convolutions being particularly error-prone. Figure~\ref{fig:bug_stats} visualizes these patterns by error type.

\begin{figure}[htbp]
\centering
\vspace{-0.3em}
\includegraphics[width=0.8\columnwidth]{figs/bug_by_error_type}
\vspace{-0.3em}
\caption{Bug distribution statistics by error type.}
\Description{A figure showing bug distribution statistics by error type.}
\label{fig:bug_stats}
\vspace{-0.5em}
\end{figure}

\subsection{Comparative Study}
\label{sec:comparison}
To quantitatively validate our approach, we conducted a comparative study against NNSmith~\cite{liu2023nnsmith}, a state-of-the-art structure-level fuzzer. We conducted five independent 4-hour fuzzing runs for each tool on the same hardware targeting PyTorch, with both tools running in identical Conda environments.

Table~\ref{tbl:comparison} summarizes the results. NNSmith generated on average $19{,}063\pm 360$ test cases and uncovered $296\pm 19$ bugs. Most of its findings are numerical mismatches rather than memory-safety issues. In contrast, \textsc{GPU-Fuzz} generated on average $51{,}860\pm 1{,}559$ test cases and uncovered $106\pm 8$ real bugs excluding out-of-memory errors, including $26\pm 5$ critical memory errors and $81\pm 7$ configuration errors. These memory errors represent severe security vulnerabilities that could result in data corruption, information leakage, or system crashes.

\begin{table}[htbp]
\centering
\caption{Comparative Results}
\label{tbl:comparison}
\footnotesize
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{NNSmith} & \textbf{GPU-Fuzz} \\
\midrule
\textbf{Test Cases Generated} & $19{,}063\ \pm\ 360$ & $51{,}860\ \pm\ 1{,}559$ \\
\textbf{Total Bugs}\textsuperscript{*} & $296\ \pm\ 19$ & $106\ \pm\ 8$ \\
\midrule
\multicolumn{3}{@{}l}{\textbf{Bug Breakdown by Type}} \\
\hspace{1em}Memory Errors & 0 & $26\ \pm\ 5$ \\
\hspace{1em}Configuration Errors & 0 & $81\ \pm\ 7$ \\
\hspace{1em}Inconsistencies & $293\ \pm\ 19$ & 0 \\
\hspace{1em}Exceptions & $3\ \pm\ 1$ & 0 \\
\midrule
\textbf{Runtime} & \multicolumn{2}{c}{4 GPU-hours each} \\
\bottomrule
\multicolumn{3}{@{}l}{\scriptsize\textsuperscript{*}GPU-Fuzz total excludes out-of-memory errors.} \\
\multicolumn{3}{@{}l}{\scriptsize\textsuperscript{**}NNSmith and GPU-Fuzz results are mean $\pm$ std over 5 independent runs.} \\
\end{tabular}%
\end{table}

\parh{Key Findings.} Our analysis reveals two critical insights. First, \textsc{GPU-Fuzz} generated nearly three times more test cases than NNSmith, demonstrating the efficiency of constraint-guided parameter fuzzing in systematically exploring the operator parameter space. Second, and more importantly, \textsc{GPU-Fuzz} uncovered $26\pm 5$ critical memory errors that pose serious security risks, while NNSmith's findings were primarily numerical precision issues that typically do not threaten memory safety. This demonstrates that \textsc{GPU-Fuzz} addresses a critical blind spot in GPU memory security testing that structure-level fuzzers frequently miss. The two approaches are complementary: NNSmith excels at uncovering compiler-related bugs and numerical inconsistencies at the network structure level, while \textsc{GPU-Fuzz} fills the gap in GPU memory security testing at the operator parameter level.

\subsection{Case Study}
\label{sec:case_study}
To illustrate the practical utility of \textsc{GPU-Fuzz}, we present a minimal proof-of-concept (PoC) for a memory corruption bug uncovered in PyTorch's \texttt{ConvTranspose2d} operator. The PoC is shown in Figure~\ref{fig:poc_code}.

\begin{figure}[htbp]
\centering
\vspace{-0.2em}
\includegraphics[width=0.7\columnwidth]{figs/code.pdf}
\vspace{-0.4em}
\caption{A minimal PoC that triggers a memory corruption bug in PyTorch's \texttt{ConvTranspose2d}.}
\Description{A code snippet showing a minimal proof-of-concept that triggers a memory corruption bug in PyTorch's ConvTranspose2d operator.}
\label{fig:poc_code}
\vspace{-0.2em}
\end{figure}

The key to triggering this bug lies in the parameter combination automatically generated by our fuzzer, particularly the extremely large stride value of \texttt{(200, 200)}. While this value is semantically valid according to PyTorch's API, it represents a corner case that is unlikely to be covered by manual unit tests. When this code is executed, the low-level oracle in our system detects a cascade of out-of-bounds global write violations, which are critical memory-safety errors. Figure~\ref{fig:poc_log} shows an excerpt from the terminal output, highlighting one such violation. The detailed error log reveals that these out-of-bounds memory accesses occur within the low-level \texttt{col2im\_kernel} CUDA kernel, which is fundamental to the transposed convolution operation. This type of silent memory corruption is a severe bug that can lead to incorrect results or unpredictable behavior without causing an explicit API-level crash. This case demonstrates the ability of \textsc{GPU-Fuzz} to systematically uncover severe, hidden bugs in mature deep learning frameworks by exploring non-trivial parameter spaces.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\columnwidth]{figs/log.pdf}
\caption{The error message for the PoC in Figure~\ref{fig:poc_code}}
\Description{An error log showing the error message for the proof-of-concept code in the previous figure.}
\label{fig:poc_log}
\end{figure}
