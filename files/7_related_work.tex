\section{Related Work}
\label{sec:related_work}
Fuzzing for DL systems has received continuous attention over the past years~\cite{shen2021comprehensive}.
To capture bugs from complicated modern DL compiler stacks, one of the effective approaches is generating valid neural network models. Works like NNSmith~\cite{liu2023nnsmith}, TVMFuzz~\cite{shen2021comprehensive}, and HirGen~\cite{ma2023fuzzing} pioneered this direction, proving effective at identifying compiler-related bugs, such as graph-level optimization issues and IR transformation errors.
Following this, other works propose testing different components of the DL stack. LEMON~\cite{wang2020deep}, for example, tests DL library implementations by generating model variants to detect inconsistencies across different libraries. More recent studies, like Orion~\cite{harzevili2025history}, specifically focus on the API layer by generating test inputs guided by historical bug patterns.
While effective, these approaches primarily operate at higher abstraction levels. They are not designed to systematically probe the low-level memory access patterns within the GPU kernels that execute the operators. Testing GPU kernels, in general, has its own line of research. GPUVerify~\cite{betts2012gpuverify} employs formal methods to verify kernel correctness by detecting data races and barrier divergence. DeepSmith~\cite{cummins2018compiler} generates random CUDA programs from scratch to stress-test the CUDA compiler~\cite{guide2020cuda} itself.

In contrast, \textsc{GPU-Fuzz} differs from this previous work. Instead of generating entire models like NNSmith~\cite{liu2023nnsmith} or entire CUDA programs like DeepSmith~\cite{cummins2018compiler}, it focuses on parameter space fuzzing of existing DL operators to uncover bugs at the kernel level.
