

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{./draw/draw.pdf}
	\caption{Execution slowdown of \tool on different benchmarks. Note that for LLM benchmarks, we measure the throughput~(tokens/s), which is higher is better; we measure other cases' execution time~(seconds), which is lower is better. PolyBench's \texttt{lu} and \texttt{gemm} cannot finish within 60 minutes under compute-sanitizer, we omit them in the figure.}
	\label{fig:runtime}
\end{figure*}

\section{Evaluation}
\label{sec:eval}

To evaluate \tool, we first benchmark its ability to detect different types of
GPU memory corruption in \S~\ref{subsec:sec_eval}. We then assess the
performance of \tool under various GPU workloads in \S~\ref{subsec:perf_eval}.

\parh{Evaluation setup.}~Our evaluation used the following software setup:
Linux 6.12.21, NVIDIA driver 570.144 and CUDA 12.8. The hardware setup
consists of an AMD Ryzen 9950X and an NVIDIA RTX 5090 with 32 GiB device memory.


\subsection{Security Evaluation}
\label{subsec:sec_eval}

\begin{table}[htbp]
	\caption{Security evaluation of \tool.}
	\label{tab:sec_eval}

	\centering
	\resizebox{\linewidth}{!}{
		\begin{threeparttable}
			\begin{tabular}{llcccccc}
				\hline
				\multicolumn{2}{c}{\textbf{Type}}     & \textbf{\#}     & \textbf{\begin{tabular}[c]{@{}c@{}}compute\\ sanitizer\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}GPU\\ Shield\tnote{1}\end{tabular}} & \textbf{cuCatch\tnote{1}} & \textbf{LMI\tnote{1}} & \textbf{\tool}              \\ \hline
				\multirow{3}{*}{\textbf{Spatial}}     & \textbf{Global} & 4                                                                    & 2                                                                      & 2                         & 4                     & 2              & \textbf{4} \\ \cline{2-8}
				                                      & \textbf{Local}  & 8                                                                    & 0                                                                      & 6                         & 5                     & 6              & \textbf{8} \\ \cline{2-8}
				                                      & \textbf{Shared} & 3                                                                    & 1                                                                      & 0                         & 1                     & 1              & \textbf{3} \\ \hline
				\multicolumn{2}{c}{\textbf{Coverage}} & /               & 20\%                                                                 & 53.3\%                                                                 & 66.7\%                    & 60\%                  & \textbf{100\%}              \\ \hline
				\multirow{4}{*}{\textbf{Temporal}}    & \textbf{UAF}    & 8                                                                    & 1                                                                      & N.A.\tnote{2}             & 5                     & 6              & \textbf{8} \\ \cline{2-8}
				                                      & \textbf{UAS}    & 4                                                                    & 3                                                                      & N.A.                      & 4                     & 0              & \textbf{4} \\ \cline{2-8}
				                                      & \textbf{IF}     & 2                                                                    & 2                                                                      & N.A.                      & 2                     & 2              & \textbf{2} \\ \cline{2-8}
				                                      & \textbf{DF}     & 4                                                                    & 4                                                                      & N.A.                      & 4                     & 4              & \textbf{4} \\ \hline
				\multicolumn{2}{c}{\textbf{Coverage}} & /               & 55.6\%                                                               & /                                                                      & 83.3\%                    & 66.7\%                & \textbf{100\%}              \\ \hline
			\end{tabular}
			\begin{minipage}{0.9\linewidth}
				\small UAF: Use-After-Free, UAS: Use-After-Scope, IF: Invalid Free, DF: Double Free.
			\end{minipage}
			\begin{tablenotes}
				\item[1]Due to the lack of public implementations of GPUShield, cuCatch and LMI, we estimated their coverage based on the description in their papers.
				\item[2]GPUShield does not detect temporal memory corruption.
			\end{tablenotes}
		\end{threeparttable}
	}
\end{table}



We evaluated \tool using a benchmark similar to those described in the papers
of cuCatch and LMI, as neither project has released their security benchmarks.
Our benchmarks, which we constructed and extended based on their descriptions,
comprise a total of 33 programs: 15 contain spatial memory corruption and 18
contain temporal memory corruption. For spatial vulnerabilities, our benchmarks include
both adjacent and non-adjacent overflows. Additionally, for local
memory overflows, we evaluate both in-frame and cross-frame overflows. As for
temporal vulnerabilities, we include cases with both immediate and delayed
access. This diverse set of testcases provides a comprehensive evaluation of the
detectability of GPU sanitizers. Our benchmarks have been released at
\cite{cusan} to support future research. \T~\ref{tab:sec_eval} summarizes the
evaluation results of existing GPU sanitizers.

\parh{Spatial corruption.}~From the \F~\ref{tab:sec_eval}, we can observe that
compute-sanitizer detects only three of the 15 spatial memory corruption
programs. It misses all non-adjacent memory overflows and has limited coverage
for the adjacent ones. LMI and GPUShield demonstrate similar coverage as both
of them rely on a mechanism that encodes the $2^n$-aligned size into the
pointer\footnote{Unlike LMI, which relies on $2^n$-aligned size for detection,
GPUShield adopts it to improve the performance.}. This approach makes them
unable to detect memory overflows that occur within the $2^n$ range. In
contrast, \tool overcomes this limitation by embedding the exact size of a
buffer into the metadata, allowing it to accurately detect spatial overflow.
While cuCatch also captures the exact bounds of memory buffers, it fails to
infer the exact bounds of local/shared memory buffers. This is because the
bound information is not available in the compiler's backend, where cuCatch is
implemented. Implemented on the LLVM frontend, \tool can easily obtain the
exact bounds of memory buffers for all memory types, thereby avoiding the
limitations of cuCatch. Thus, \tool achieves the best coverage for spatial
corruption among all sanitizers.


\parh{Temporal corruption.}~As for temporal memory corruption,
compute-sanitizer detects 10 out of 18 cases; it misses all delayed UAF bugs
and one immediate UAS~(i.e., dereferencing a local pointer immediately after
the function returns). GPUShield does not offer protection against temporal
memory corruption and was excluded from the evaluation. cuCatch detects delayed
UAFs probabilistically by assigning one of 256 tags to each allocation.
However, when many buffers (>256) are allocated between the use-site and
free-site, tag collisions become likely. In our evaluation, cuCatch failed to
detect three delayed UAFs due to tag collision. LMI, on the other hand,
provides deterministic detection of temporal memory corruption via metadata
invalidation. Yet, since its metadata is embedded solely in the pointer, the
invalidation of the original pointer does not propagate to the pointers that
were copied before the free-site. As a result, LMI misses two UAFs and all UASs
where copied pointers are involved. In contrast, \tool uses VA-randomization to
detect delayed UAFs, which provides stronger heuristic protection~(i.e.,
$2^{41-A}$ as described in \S~\ref{subsec:temporal_meta}). This makes the tag
collision seen in cuCatch highly unlikely. Moreover, unlike LMI, \tool embeds
the metadata in both the pointer and the buffer, ensuring the
invalidation is visible to all use-site, including those of copied pointers.
Therefore, \tool achieves the best coverage in detecting temporal
corruption.


\subsection{Performance Evaluation}
\label{subsec:perf_eval}

We evaluated \tool's performance across a diverse set of 44 testcases,
summarized in \T~\ref{tab:perf_eval}. Our evaluation incorporates both
established GPU benchmark suites, such as Rodinia, Tango, and
PolyBench-GPU~\cite{polybenchgpu,tango,rodinia}, and two popular large language
models (LLMs) to assess performance on real-world workloads: LLaMA2 (7B) and
LLaMA3 (8B)~\cite{llama2,llama3}. All reported overheads are the average of
five iterations.


\begin{table}[htbp]
	\caption{Benchmarks used to evaluate \tool.}
	\label{tab:perf_eval}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{ccl}
			\hline
			\textbf{Suite} & \textbf{\#} & \multicolumn{1}{c}{\textbf{Testcases}}                                                                                                                                                                                                         \\ \hline
			Rodinia        & 17          & \begin{tabular}[c]{@{}l@{}}b+tree, bfs, backprop, lavaMD, bfs, gaussian\\ pathfinder, srad, particlefilter\\ lud, nn, particle\_naive, particle\_float\\ sradv1, sradv2,hotspot, hotspot3d\\ dwt2d, heartwall, needle, pathfinder\end{tabular} \\ \hline
			PolyBench      & 19          & \begin{tabular}[c]{@{}l@{}}conv2d, conv3d, adi, bicg, covar, \\ gramschmidt, jacobi1d, jacobi2d\\ fdtd, gemver, mvt, 2mm, 3mm\\ atax, corr, doitgen, gemm, gesummv, lu\end{tabular}                                                            \\ \hline
			Tango          & 6           & AlexNet, CifarNet, GRU, LSTM, ResNet, SqueezeNet                                                                                                                                                                                               \\ \hline
			LLM            & 2           & LLaMA2, LLaMA3                                                                                                                                                                                                                                 \\ \hline
		\end{tabular}
	}
\end{table}

\subsubsection{Runtime Overhead}
\parh{Execution slowdown.}~\F~\ref{fig:runtime} presents the performance
overhead of \tool and compute-sanitizer. On average, \tool introduces a 13\%
overhead to the execution time and an 11\% drop on the throughput of LLMs. In
contrast, the average overhead of compute-sanitizer is substantially higher; it
imposes a 15$\times$ overhead and reduces LLM throughput by 98\%. For worst
case scenarios, compute-sanitizer imposes a 153$\times$ overhead on the execution time
and a 98.5\% reduction on LLM throughput. \tool, on the other hand, incurs a
maximum overhead of 83\% on the execution time~(observed on \texttt{gemm}) and
an 18\% drop on LLaMA3's throughput. This maximum overhead of 83\% is observed
in the \texttt{gemm} testcase from PolyBench, which includes a naive
implementation of matrix multiplication with sparse memory access. We attribute
this overhead to its sparse memory access pattern, which has a negative impact
on the cache efficiency. On the same \texttt{gemm} testcase, compute-sanitizer
incurs an overhead of 153$\times$, which partially supports our analysis. We believe
this situation rarely happens in real-world applications with highly optimized
CUDA kernels~(e.g., LLaMAs)

Other related works, while valuable, are not directly comparable for practical
reasons. The software sanitizer cuCatch, for instance, reports a 19\% average
and 3.1$\times$ maximum overhead; as cuCatch's evaluation suites are not
open-source, we cite its published results\footnote{We contacted the authors of
cuCatch for this information, but have not received a response.}. In another
domain, hardware-assisted schemes like LMI show promising potential with a
minimal 0.2\% overhead. However, their effectiveness relies on specialized
hardware that is unavailable in current commercial GPUs, limiting their
immediate applicability. Our work, in contrast, focuses on a practical solution
designed for direct use and evaluation on today's hardware.

\parh{Memory overhead.}~Beyond execution time, memory consumption is another
critical metric for GPU sanitizers, with details summarized in
Table~\ref{tab:mem}. We omit compute-sanitizer as it is a closed-source tool
with no details on its design. LMI's memory overhead is a byproduct of its
requirement that all memory be $2^n$-aligned, meaning the specific overhead is
payload-dependent. cuCatch and \tool's memory overhead, on the other hand,
consists of a fixed part and a scalable part. cuCatch introduces a fixed
overhead of 160\,MiB from its initial metadata structure. In addition, it
imposes a scalable overhead of 12.5\%, requiring 32 bits of metadata for every
32 bytes of allocated memory. \tool allocates a fixed memory for the stack
epoch, which is only 16.5 MiB~(see \S~\ref{subsec:temporal_meta}). As for
scalable part, \tool only attaches an 8-byte in-band size for each
allocation.(see \S~\ref{subsec:spatial_meta}).
%
Though \tool enforces $2^n$-alignment on allocated buffers, this alignment is
applied only to their VAs. Unlike LMI, \tool does not allocate additional
physical memory to fill the padding region; the physical memory is mapped only
to the actually used portion. Consequently, this alignment introduces no extra
memory consumption. 

\begin{table}[htbp]
	\caption{Memory overhead of different GPU sanitizers.}
	\label{tab:mem}
	\centering
	\resizebox{.8\linewidth}{!}{
		\begin{tabular}{ll}
			\hline
			\textbf{Tool} & \textbf{Memory overhead}                       \\ \hline
			LMI           & $2^n$-aligned fragmentation                    \\ \hline
			cuCatch       & Fixed part: 160 MiB; Scale part: 12.5\%        \\ \hline
			\tool         & Fixed part: 16.5 MiB; Scale part: 8 bytes/alloc \\ \hline
		\end{tabular}
	}
\end{table}

Apart from the above analysis, we also measured the memory overhead of \tool
and other sanitizers. \T~\ref{tab:mem_overhead} summarizes the results. Since
the implementations of cuCatch and LMI are not available, we first recorded the
size of each allocation in the test program, and then calculated the overhead
according to the allocation profile and the designs described in their papers.
We omit compute-sanitizer in \T~\ref{tab:mem_overhead}, as there is neither its
design details nor an accurate way to track its memory usage (it does not use
\texttt{cudaMalloc}). Nonetheless, we estimated its memory usage using
\texttt{nvidia-smi}, observing an overhead ranging from 200\,MiB to 600\,MiB,
depending on the specific test program.


\begin{table}[htbp]
	\caption{Memory overhead of GPU sanitizers.}
	\label{tab:mem_overhead}
	\centering
	\resizebox{.7\linewidth}{!}{
		\begin{tabular}{rccc}
			\hline
			                                                                      & \textbf{cuCatch}                                                            & \textbf{LMI}                                                        & \textbf{\tool}                                                         \\ \hline
			\textbf{\begin{tabular}[c]{@{}r@{}}Max. Rel.\\ Overhead\end{tabular}} & \begin{tabular}[c]{@{}c@{}}5,121.5$\times$\\ \texttt{jacobi1d}\end{tabular} & \begin{tabular}[c]{@{}c@{}}2$\times$\\ \texttt{needle}\end{tabular} & \begin{tabular}[c]{@{}c@{}}528$\times$\\ \texttt{jacobi1d}\end{tabular} \\ \hline
			\textbf{\begin{tabular}[c]{@{}r@{}}Max. Abs.\\ Overhead\end{tabular}} & \begin{tabular}[c]{@{}c@{}}3.79 GiB\\ \texttt{llama2}\end{tabular}          & \begin{tabular}[c]{@{}c@{}}6.89 GiB\\ \texttt{llama2}\end{tabular}  & \begin{tabular}[c]{@{}c@{}}16.51 MiB\\ \texttt{llama3}\end{tabular}     \\ \hline
			\textbf{\begin{tabular}[c]{@{}r@{}}Average\\ Overhead\end{tabular}}   & \begin{tabular}[c]{@{}c@{}}0.73 GiB\\ 15\%\end{tabular}                     & \begin{tabular}[c]{@{}c@{}}1.10 GiB\\ 23\%\end{tabular}             & \begin{tabular}[c]{@{}c@{}}16.5 MiB\\ 0.3\%\end{tabular}               \\ \hline
		\end{tabular}
	}
\end{table}

\T~\ref{tab:mem_overhead} reports the maximum and average memory overhead for
each sanitizer. From this table, LMI demonstrates the best performance in terms
of relative overhead, which is 2$\times$, while cuCatch's and \tool's are
5,121.5$\times$ and 528$\times$, respectively. This superior relative
performance by LMI, however, is attributed to its $2^n$-aligned memory policy,
which inherently bounds its worst-case relative overhead at 2$\times$. The
\texttt{jacobi1d} is also a special case; it allocates a very small amount of
memory~(32\,KiB), allowing the fixed overheads of cuCatch~(160\,MiB) and
\tool~(16.5\,MiB) to dominate its relative overhead. Regarding absolute
overhead, both cuCatch and LMI incur significant memory overheads of 3.79\,GiB
and 6.89\,GiB on the LLM benchmark, whereas \tool only requires 16.51\,MiB of
additional memory. Concerning average overhead, LMI performs the worst,
incurring an average overhead of 23\% and 1.10\,GiB of additional memory.
cuCatch has a moderate average overhead of 15\% with 0.73\,GiB. \tool, on the
other hand, has a negligible average overhead of only 0.3\% and requires
merely 16.5\,MiB of additional memory. 

For LLMs, the memory overhead from cuCatch and LMI might trigger out-of-memory
errors, causing functional LLM applications to crash unexpectedly. In contrast,
\tool adds negligible memory overhead, making such errors highly unlikely. This
property is crucial for LLMs, where memory usage is substantial and sensitive
to even modest increases.

\subsubsection{Occupancy and Divergence}
Occupancy and divergence are two important concepts in GPU programming.
Occupancy refers to the number of active warps per SM, which depends on the
hardware resources used by each thread and reflects kernel parallelism. Low
occupancy indicates that the GPU is underutilized, potentially leading to
performance degradation. We used \texttt{nsight}~\cite{nsight} to measure the
occupancy of CUDA kernels before and after \tool's instrumentation. Across the
113 CUDA kernels analyzed (one GPU application may contain multiple kernels),
only 13 showed an occupancy drop of more than 10\% after instrumentation, six
of which involve matrix multiplication (e.g., \texttt{gemm}). The largest drop,
50\%, was observed in the \texttt{lavaMD} testcase from Rodinia. Notably,
occupancy changes did not directly correlate with \tool's runtime overhead:
\texttt{gemm} had the highest overhead of 83\% but only a 24\% occupancy drop,
while \texttt{lavaMD} had a negligible overhead (<1\%) but a 50\% drop. This
indicates that \tool's overhead primarily comes from the extra time spent on
instrumented instructions, rather than from reduced kernel occupancy.

Divergence refers to the situation where threads in a warp take different
execution paths on the same branch, causing the divergent instructions to be
executed serially, leading to slowdown. Though \tool inserts additional
conditional branches, these branches are solely for checking the validity of
the memory access. In other words, divergence in these checks indicates memory
corruption, and therefore the program should be terminated. Therefore, such
divergence is not expected in normal execution. To confirm this, we measured
the divergence rate before and after \tool's instrumentation, and observed no
measurable increase.


\subsubsection{Optimized Checks}
In this section, we evaluate the effectiveness of \tool's optimizations by
measuring both the number of checks removed and the resulting performance
improvement. As shown in \F~\ref{fig:opt}, applying the three optimization
rules described in \S~\ref{subsec:opt} eliminates, on average, near 20\% of checks
across the 44 GPU programs used in our performance evaluation. These results
demonstrate the effectiveness of the proposed rules in identifying and removing
redundant checks.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{./draw/opt.pdf}
	\caption{Percentage of checks optimized by \tool.}
	\label{fig:opt}
\end{figure}

\parh{Performance gain.}~We also measured the performance improvement from
\tool's optimizations. On average, these optimizations reduce the execution
time by 3.5\% and improve LLM's throughput by nearly 2\%. The most significant
performance gain is observed in the \texttt{lud}, where \tool's optimizations
reduces the execution time by over 60\%. We attribute this to the large number
of shared memory accesses in \texttt{lud}; removing the redundant checks allows
the compiler to better optimize the shared memory access pattern, leading to
substantial performance improvement.
