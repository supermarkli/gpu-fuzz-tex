\section{System Design}
\label{sec:design}

This section details the architecture of \textsc{GPU-Fuzz}. As illustrated in Figure~\ref{fig:arch}, the system consists of three main phase, operator modeling, constraint-based testcase generation and cross-framework execution.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\linewidth]{figs/arch}
	\caption{The architecture of the \textsc{GPU-Fuzz} system.}
	\Description{A diagram illustrating the architecture of the GPU-Fuzz system. }
	\label{fig:arch}
\end{figure}

\subsection{Operator modeling.}
\label{sec:modeling}

\textsc{GPU-Fuzz} models GPU operators through an abstraction layer that captures their parameter spaces and shape relationships. Each operator family (e.g., convolution, pooling, activation) is represented by a unified model that defines the interface for input/output shapes and parameter constraints.
For operators with complex shape transformations, such as convolutions and pooling operations, we introduce symbolic variables using the Z3 SMT solver~\cite{de2008z3} to represent operator parameters. The abstraction allows us to model operators uniformly across different frameworks while preserving framework-specific semantics.

\subsection{Constraint-based Testcase Generation}
\label{sec:testcase}

\parh{Constraint solving.}\textsc{GPU-Fuzz} encodes operator semantics as constraints over symbolic variables through two layers: generic constraints and operator-specific constraints.

Generic constraints ensure basic validity: operator parameters (e.g., tensor dimensions) must be reasonable positive integers, and total I/O tensor memory must not exceed GPU limits.

Operator-specific constraints capture the mathematical relationships between input and output shapes. Figure~\ref{fig:constraint_modeling} illustrates that for a convolution operator, the output size along each spatial dimension follows the formula~\cite{dumoulin2016guide}:
\[
H_{\text{out}} = \frac{H_{\text{in}} + 2P - D(K-1) - 1}{S} + 1
\]
where $H_{\text{in}}$ and $H_{\text{out}}$ denote the input and output sizes, $P$ is padding, $D$ is dilation, $K$ is kernel size, and $S$ is stride. Additional constraints ensure semantic correctness, such as requiring the input size to be larger than the kernel size.

\textsc{GPU-Fuzz} uses Z3's SMT solver~\cite{de2008z3} to find satisfying assignments for all symbolic variables. The solver returns a model that assigns concrete values to each parameter, which are then used to instantiate the operator for testcase generation.

\begin{figure}[htbp]
	\centering
	\hspace*{-0.2cm}\includegraphics[width=0.9\columnwidth]{figs/constraint}
	\caption{Constraint modeling for convolution operators.}
	\Description{A diagram illustrating the constraint modeling process for convolution operators.}
	\label{fig:constraint_modeling}
\end{figure}

\parh{Parameter space exploration.}To systematically explore the parameter space, \textsc{GPU-Fuzz} employs an iterative constraint-guided search strategy. After obtaining a solution from the solver, the system randomly selects one symbolic variable from the current model and adds constraints that exclude its current value. Specifically, the system adds both a hash-based constraint and a direct value constraint: the hash constraint helps the solver efficiently prune value ranges with better dispersion, while the direct constraint ensures the new solution differs from the current one. This dual-constraint approach improves solver efficiency by providing better value distribution guidance, while the incremental constraint addition forces the solver to find different solutions in subsequent iterations, effectively guiding the search toward unexplored regions of the parameter space. Figure~\ref{fig:exploration} illustrates this process.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\linewidth]{figs/exploration}
	\caption{An example of the iterative parameter space exploration. At each step, a parameter is randomly selected and a new constraint that excludes its current value is incrementally added to guide the search for the next solution.}
	\Description{A diagram illustrating the iterative parameter space exploration process.}
	\label{fig:exploration}
\end{figure}

\subsection{Cross-framework Execution}
\label{sec:execution}

\textsc{GPU-Fuzz} detects framework-specific GPU bugs by executing generated test cases across multiple deep learning frameworks (PyTorch~\cite{paszke2019pytorch}, PaddlePaddle~\cite{ma2019paddlepaddle}, and TensorFlow~\cite{abadi2016tensorflow}). Its execution pipeline involves materializing abstract operator representations into framework-specific API calls, handling nuances like parameter naming and data formats. An execution controller orchestrates this by iteratively generating configurations, materializing, and executing them on GPUs. NVIDIA Compute Sanitizer~\cite{nvidia2023compsan} wraps each execution to monitor for GPU memory errors and kernel failures (e.g., out-of-bounds). This cross-framework approach facilitates differential testing, identifying inconsistencies and framework-specific bugs by executing semantically equivalent test cases across different frameworks.
