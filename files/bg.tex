\section{Background and Prior Work}
\label{sec:bg}

\subsection{GPU Architecture}

\F~\ref{fig:gpu_mem} illustrates the architecture of a typical GPU. Streaming
Multiprocessors~(SMs) are the basic compute units of a GPU. Each SM contains
multiple computing cores to execute threads in parallel. A GPU can have tens to
hundreds of SMs, enabling it to execute thousands of threads simultaneously.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{figs/gpu_mem.pdf}
	\caption{GPU memory architecture.}
	\label{fig:gpu_mem}
\end{figure}


To achieve low-latency memory access, GPUs employ different types of memory to
meet diverse latency and bandwidth requirements. As shown in
\F~\ref{fig:gpu_mem}, the GPU memory system comprises a hierarchy of shared,
local, and global memory.

\parh{Shared memory} is a small but fast on-chip memory region shared by all
threads within the same SM. Unlike off-chip memory, shared memory can be
accessed with low latency; it is 100$\times$ faster than off-chip
memory~\cite{sharedmem}. This speed comes at the cost of limited capacity, for
example, 64\,KB per SM. Because of its low latency, the shared memory is often
used to store data that requires frequent access to avoid accessing costly
off-chip memory. Despite its name, shared memory is a scratchpad; some GPUs
allow configuring the ratio between shared memory and L1 cache, but the total
size is still limited.

\parh{Local memory} is an area within off-chip memory that is private to each
thread. It stores the thread's stack and is therefore referred to as stack
memory. Similar to the stacks on CPUs, local memory is not persistent and is
freed once all the threads within the same function~(i.e., kernel) exit.
The allocation of local memory is implicitly managed by NVIDIA's closed-source
driver, meaning it cannot be directly controlled from a low-level
perspective~(e.g., changing its address mapping).

\parh{Global memory} also resides in the same off-chip memory as local memory,
but can be managed from the CPU side. Global memory is allocated through
runtime APIs~(e.g., \texttt{cudaMalloc}). Unlike local memory, which is freed
once its threads exit, global memory is persistent and accessible by all
threads across all GPU functions until it is explicitly freed~(e.g.,
\texttt{cudaFree}). Additionally, NVIDIA partially open-sources the kernel
driver~\cite{opennv}; its exposed APIs allow developers to customize the
allocation of global memory. This capability enables \tool to implement an
MMU-based pointer tagging scheme, which will be discussed in
\S~\ref{subsec:tag}.

\parh{Cache hierarchy.}~As illustrated in \F~\ref{fig:gpu_mem}, the GPU has a
cache hierarchy to reduce the latency of off-chip memory. Each SM has a private
L1 cache and a shared L2 cache. Furthermore, to reduce the latency of page
table translation, each SM also has a private L1/L2 TLB and a shared L3 TLB.
This hierarchical design reduces the latency of memory access from the
numerous threads. However, due to its highly parallel nature, a GPU
application must be carefully designed~(e.g., avoiding accesses to pointers
that are widely dispersed across memory) to prevent TLB thrashing. In our
observation, retrieving sanitizer metadata from a shadow memory region as in
cuCatch~\cite{tarek2023cucatch} and AddressSanitizer~(ASAN)~\cite{asan} is
likely to cause this issue. To address this, \tool embeds metadata in-band at
the beginning of each buffer, a design choice well-suited to the GPU's cache
architecture.


\subsection{Pointer Tagging}
\label{subsec:tag}
\tool relies on pointer tagging to retrieve the metadata~(see
\S~\ref{subsec:overview}); we discuss its implementation on GPUs.

\parh{MMU-based global pointer tagging.}~Though major CPU vendors such as Intel
have announced their pointer tagging features~(e.g., Intel LAM~\cite{lam}), no
equivalent features exist on GPU. Fortunately, NVIDIA's open-source driver
exposes interfaces for managing the allocation of global memory, which allow
manipulation of virtual addresses for global buffers via the GPU MMU. We
thereby leverage the MMU to customize the upper bits of the virtual
address~(VA) to encode metadata, such as tags.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{figs/gpu_mmu.pdf}
	\caption{GPU MMU page table format.}
	\label{fig:gpu_mmu}
\end{figure}

For instance, \F~\ref{fig:gpu_mmu} shows the page table format of
Blackwell\footnote{NVIDIA's GPUs have different page table designs across
generations; we provide this example for illustration.} GPUs~\cite{blackwell}.
As shown in the figure, a modern GPU supports multiple page sizes, including
\ding{192}\,4\,KB, \ding{193}\,64\,KB and \ding{194}\,2\,MB. Translation for
all these page sizes follows the same traversal process, driven by bits 56
through 21 of the VA. The page directory 5~(PD5) has only two entries, which
store the addresses of PD4 tables and are indexed by bit 56 of the VA. Page
directories PD4 through PD1 each contains 512 entries, indexed by nine bits of
the VA. PD0 differs, containing only 256 entries indexed by eight bits of the
VA. Each entry in PD0 can either be viewed as one 16-byte entry or two 8-byte
entries. In the former case, the 16-byte entry directly stores the address of a
2\,MB page frame~(\ding{192} in \F~\ref{fig:gpu_mmu}). In the latter case, the
lower half 8-byte entry points to a 64\,KB page frame~(\ding{193} in
\F~\ref{fig:gpu_mmu}) and the upper half 8-byte entry points to a 4\,KB page
frame~(\ding{194} in \F~\ref{fig:gpu_mmu}).

These PDx tables govern the translation result of a VA; therefore, by
configuring the page table, we can embed metadata within the higher bits of the
VA, effectively making these bits part of the pointer's tag. For example, \tool
enforces all objects of 256\,B to be allocated in the VA region with bits
[46:41]$=log(256)-log(16)+1=6$, with 16\,B being the minimal allocation size.
This approach enables flexible tagging of global buffer pointers through
judicious page table configuration.

\parh{Local/shared pointer tagging.}~For objects allocated in the local/shared
memory, their virtual addresses cannot be arbitrarily manipulated for tagging,
unlike those in global memory. This limitation arises because these addresses
are exclusively managed by NVIDIA's closed-source runtime. Instead, we adopt a
pseudo-pointer based approach. \F~\ref{fig:local_tag} presents a code snippet
that illustrates the tagging process of local memory pointers. In this snippet,
we allocate a local memory buffer of 24 bytes with 32-byte alignment~(line 1).
Subsequently, at line 2, a pseudo-pointer \texttt{\%tagged} is constructed by
embedding the necessary metadata~(e.g., its $2^n$ aligned size) into the
original pointer's value. As \texttt{\%tagged} is not a directly dereferencable
memory address, the tagging procedure must be reversed at line 4 to recover the
original, valid pointer \texttt{\%raw}. This raw pointer is then used to access
the memory buffer at line 5. This technique allows local/shared memory buffers
to be effectively tagged, albeit with a slight computational overhead due to
the additional arithmetic operations.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{figs/local_tag.pdf}
	\caption{Local/shared memory tagging.}
	\label{fig:local_tag}
\end{figure}

\noindent\underline{Compatibility.}~A potential compatibility arises if a
``pseudo-pointer'' is passed to an external function, as the function would be
unable to dereference it directly. While such scenarios are infrequent in
typical CUDA applications (specifically, passing a local/shared pointer to an
external function), \tool mitigates this by automatically untagging such
pointers before they are passed to external functions.

\newcommand*\emptycirc[1][.8ex]{\tikz\draw (0,0) circle (#1);}
\newcommand*\halfcirc[1][.8ex]{%
	\begin{tikzpicture}
		\draw[fill] (0,0)-- (90:#1) arc (90:270:#1) -- cycle ;
		\draw (0,0) circle (#1);
	\end{tikzpicture}}
\newcommand*\fullcirc[1][.8ex]{\tikz\fill (0,0) circle (#1);}

\begin{table*}[htbp]
	\centering
	\caption{Comparison between previous GPU memory safety solutions and \tool.}
	\label{tbl:sum}
	\resizebox{.9\linewidth}{!}{%
		\begin{tabular}{lccccccc}
			\hline
			\textbf{Name}                             &
			\textbf{Base}                             &
			\textbf{Mechanism}                        &
			\textbf{Spatial}                          &
			\textbf{Temporal}                         &
			\textbf{Deployability}                    &
			\textbf{Perf. Overhead}                   &
			\textbf{Mem. Overhead}                                                                                                                      \\ \hline
			\textbf{GPUShield}~\cite{lee2022shield}   & HW & Tagging  & \fullcirc & \emptycirc & \ding{56} & 1\%           & None                       \\ \hline
			\textbf{IMT}~\cite{imt}                   & HW & Tagging  & \halfcirc & \emptycirc & \ding{56} & 4\%           & None                       \\ \hline
			\textbf{LMI}~\cite{lee2025lmi}            & HW & Aligning & \halfcirc & \halfcirc  & \ding{56} & 0.2\%         & $2^n$-fragmentation        \\ \hline
			\textbf{GMOD}~\cite{di2018gmod}           & SW & Redzone  & \halfcirc & \emptycirc & \ding{52} & 2.9\%         & 12\,B/buf                  \\ \hline
			\textbf{clArmor}~\cite{clarm}             & SW & Redzone  & \halfcirc & \emptycirc & \ding{52} & 9.6\%         & 4\,B/buf                   \\ \hline
			\textbf{compute-sanitizer}~\cite{compsan} & SW & Redzone  & \halfcirc & \halfcirc  & \ding{52} & 1,400\%       & Unknown                    \\ \hline
			\textbf{cuCatch}~\cite{tarek2023cucatch}  & SW & Tagging  & \halfcirc & \halfcirc  & \ding{56} & 19\%          & 160\,M + 12.5\%            \\ \midrule[1.5pt]
			\textbf{\tool}                            & SW & Hybrid   & \fullcirc & \fullcirc  & \ding{52} & \textbf{13\%} & \textbf{16.5\,M + 8\,B/buf} \\ \hline
		\end{tabular}%
	}
\end{table*}


\subsection{Memory Safety Solutions on GPUs}
\label{subsec:first_rw}

Memory safety is a long-standing problem. This section reviews various
memory safety solutions designed for GPUs. \T~\ref{tbl:sum} summarizes these
approaches. Though some tools (e.g., GPUShield) might not be formally named
``sanitizers'', we categorize them as such for simplicity, given their common
function of detecting memory corruption.

\parh{Redzone-based.}~Redzone is a widely used technique to detect memory
corruption. A redzone-based sanitizer typically allocates a shadow memory
region to track the status of the main memory and checks if the accessed memory
has been flagged as ``redzone'' during runtime. An example of a redzone-based
sanitizer is NVIDIA's compute-sanitizer~\cite{compsan}. It relies on binary
rewriting to insert checks before each memory access, and thus introduces
a considerable performance penalty due to the additional instructions inserted.
As shown in \T~\ref{tbl:sum}, our evaluation shows that compute-sanitizer leads
to an average slowdown of 15$\times$. Furthermore, as this approach primarily
places redzones between buffers, it cannot detect overflows that occur from one
buffer into an adjacent, valid buffer (e.g., an overflowed access from buffer A
into buffer B).

Other than compute-sanitizer, there are also GPU sanitizers like GMOD and
clArmor~\cite{di2018gmod,clarm} that are canary-based. By inserting a guard
value at the end of each memory allocation, these sanitizers detect memory
corruption by checking if the guard value is modified. As \T~\ref{tbl:sum}
shows, though this simplified approach significantly reduces the performance
overhead compared to compute-sanitizer, it also loses the capability to detect
temporal memory corruption~(e.g., use-after-free) and provides even worse
detection accuracy; any overflow that skips the guard value is not detected.

\parh{Tagging-based.}~Tagging-based sanitizers attach a tag to each pointer.
When memory access occurs, the sanitizer retrieves the metadata associated
with the tag and checks if the access is valid. For instance, NVIDIA's
cuCatch~\cite{tarek2023cucatch} embeds a tag into each pointer and queries a
two-level structure for the valid bounds of the buffer. Built upon the compiler's
backend, cuCatch shows superior performance~(19\% overhead as shown in \T~\ref{tbl:sum})
compared to approaches that rely on binary rewriting~(e.g., compute-sanitizer).
However, relying on the backend restricts it from obtaining accurate bounds,
as these are only available on the frontend; this leads to inaccurate detection.
Moreover, cuCatch's tags could potentially be tampered with as cuCatch directly
encodes the tag into the pointer yet does not check if a pointer arithmetic
overwrites it. GPUShield~\cite{lee2022shield} adopts a similar approach to
cuCatch and uses a customized hardware component to perform the bounds
check. Though the customized hardware reduces the runtime overhead, it also
prevents GPUShield from being deployed on commercial GPUs, where the hardware
is not modifiable. IMT~\cite{imt} is another hardware-based solution that uses
ECC metadata as tags. Similar to GPUShield, IMT also requires hardware
modification and therefore faces similar deployment challenges as GPUShield.
Therefore, we mark all the hardware-based solutions in \T~\ref{tbl:sum} as
undeployable. Moreover, IMT does not specifically target memory safety but
instead provides a general approach for pointer tagging.

\parh{Aligning-based.}~Aligning-based sanitizers take a different approach
compared to the above ones. Instead of performing bound checking at memory
access, they check pointers' bounds during arithmetic instructions. Such approaches enforce
the alignment of the allocated memory to $2^n$ and encode the alignment $n$ into
the pointers. During the pointer arithmetic, the sanitizer checks if the
resulting pointer is still within the $2^n$ bounds and terminate the program if it is
not. LMI~\cite{lee2025lmi} is a recent work that adopts this approach on GPUs.
LMI encodes the alignment information into the pointers and modifies the ALUs to restrict
pointer arithmetic. However, like other pointer aligning schemes, LMI
misses overflow that occurs within the $2^n$ alignment and introduces
substantial memory waste due to fragmentation. Furthermore, LMI has only
limited support for temporal memory corruption~(e.g., use-after-free); it marks
a pointer as invalid upon the free-site and thus cannot detect corruption if
the pointer is copied prior to the free operation. Lastly, similar to
GPUShield, LMI also requires modification of the GPU hardware, which limits its
deployment on commercial GPUs. Consequently, we mark its detectability of both
types of memory corruption as partial, and mark it as undeployable, in
\T~\ref{tbl:sum}.


\parh{Summary.}~\T~\ref{tbl:sum} \tool with current memory safety solutions on
GPUs. In this table, we can see that existing GPU memory safety solutions
either offer incomplete detection capability or can only be deployed with
modified hardware/proprietary software. \tool, on the other hand, is a
software-based solution that can be deployed on commercial NVIDIA GPUs.
Furthermore, by adopting a hybrid design of pointer tagging and in-band
metadata, \tool delivers comprehensive detection for both spatial and temporal
memory corruption.
